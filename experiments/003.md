# GPU sequence models

Building on the positive results from the resnet18 overfit experiment. We explore the capacity of 2 different sequential neural networks to learn the same task as in experiment 002:
- The first one is a convolutional LSTM where all usual LSTM operations have reen replaced with a convolution.
- The second one is a 3d convolutional model padded to predict the future

### Task
Learn to predict future states and actions from past states and actions.
One timestep is the tuple (state, action).

Note:
- The game is Settlers of Catan
- The history length is 16
- The future length is 4
- Input are represented as a tensor of size: `[batch_size, seq * (C_s + C_a), H, W]`
- Outputs are represented as a tuple: `([batch_size, seq * C_s, H, W], [batch_size, seq, C_a])`

### Hypothesis
The 2 sequential models can learn the raw data mapping between past and future states for a small amount of data.
The data is generated from a fixed simulator of the SOC game with a fixed set of 4 robot players.
The input is kept as is but the output is separated per types (categorical, binary, float, etc.)


### Results
The hypothesis seems false.

But sequential models are known to be trained with difficulty.
I believe some HP tuning is necessary before jumping to conclusions.
Since I already have an interesting model for prediction I'll postpone this work.