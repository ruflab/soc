# Testing the fusion module

This experiement explore if the fusion module is able to improve the prediction capacity of our model for the states relying on text information.

### Task
Learn to predict the next state and actions from past states and actions.
One timestep is the tuple (state, action).

Note:
- The game is settlers of catan
- The history length is 16
- The future length is 1
- We are especially looking a accuracy after trade actions. Those states contain meaningfull chat information that should be fusionned with the latent state of the game observation to improve prediction capacities for this particular state-action.

It is interesting to note that without chat information the prediction is stochastic as we do not know what has been traded and have to guess it. On the other hand when chat is added, the prediction becomes deterministic has the chat negotiations contains all there is to know about the trade.

#### On Bert
This experiment is using a pretrained Bert model and embedding to extract interesting features. We do not finetune Bert, instead we use it only as a feature extractor.
As mentionned in the [original paper](https://arxiv.org/pdf/1810.04805.pdf). They are multiple ways to select the interesting feature. Based on Table 7, we will simplify ourselves by using the last hidden layer features. Those consists into contextualised tokens. This means that one batch of inputs is preprocessed to end up as batch of sequence of contextualised tokens.
Those should them be fed into projection model to extract the relevant information used for potential fusion modules.

#### On the Bert feature extractor
1. We will start our experiment with the most simple way to merge the contextualised tokens: a mean.
2. Then, as in the paper, we will try a 2 hidden-layer Bidirectionnal LSTM

#### On fusion modules
1. We will grow the fusion module complexity step by step.
First we will ensure that we can extract need information from the text, and use a simple concatenation operation. (No fusion actually happen).
2. We will use simple 2 hidden layer NN to fuse information from the game and the text.
3. We will text a 2 hidden layer transformer layer
4. We might want to merge the Bert feature extractor module and the fusion module into one

### Hypothesis
A fusion module can fusion information from text and tabular data to improvate the model predictive capacity.

### Results
