# Finding a fusion module

This experiment explores different fusion module. We will explore multiple architectures with and without fusion, with and without learning parameters for the text, etc.
The goal is to find an architecture capable of using textual information to improve representations.

### Task
Learn to predict future states and actions from past states and actions.
One timestep is the tuple (state, action, text).

Note:
- The game is Settlers of Catan
- The history length is 1
- The future length is 1
- Input are represented as a tensor of size: `[batch_size, 1 * (C_s + C_a), H, W], [batch_size, 1, text_token_seq, F_bert]`
- Outputs are represented as a tuple: `([batch_size, 1 * C_s, H, W], [batch_size, 1, C_a])`

We are especially looking at the accuracy after trade actions. Those are the only states containing meaningful textual information about the game.

It is interesting to note that without chat information the prediction is stochastic as we do not know what has been traded and have to guess it. On the other hand, when chat negotiations are added, the prediction becomes deterministic. The chat negotiations contain all there is to know about the trade.

#### On Bert
This experiment is using a pre-trained Bert model and embedding to extract interesting features. We do not finetune Bert, instead, we use it only as a feature extractor.
As mentioned in the [original paper](https://arxiv.org/pdf/1810.04805.pdf).

They are multiple ways to select interesting features. Based on Table 7, we will simplify ourselves by using the last hidden layer features. Those consist into contextualised tokens: one batch of sequences of inputs is preprocessed to end up as a batch of sequences of contextualised tokens.

#### On the Bert feature extractor
1. We will start our experiment with the most simple way to merge the contextualised tokens into a fixed-size vector: a mean operation.
2. Then, as in the paper, we will try a 2 hidden-layer Bidirectional LSTM to accomplish this task.

#### On fusion modules
1. We will grow the fusion module complexity step by step. First, we will ensure that we can extract needed information from the text, and use a simple concatenation operation. (No fusion actually happen).
2. Then, We will use a simple 2 hidden layer NN to fuse information from the game and the text.
3. We will also look into the residual variation.


#### Merging exctraction and fusion
We will investigate more complex architectures, namely Transformer and the new Hopfield layer which are suited to extract and fusion information in one step.
1. We will look into the transformer architecture
2. We will use the Hopfield layer.

#### More possibilities
If this experiment ends up not working, we can try to finetune Bert and use it (using the CLS token) directly as a complete feature extractor.

### Hypothesis
One of the architectures above is an efficient fusion module which can fusion information from the Bert model and the ResNet18 model. This fusion should impact prediction accuracy in a positive way.

### Results
The hypothesis is true: A BiLSTM + residual FF fusion works very nicely.

To have detail results of experiments, please see the Google Slides presentation [here](https://docs.google.com/presentation/d/1MKxizuQflOzxMjbv_sUYUOTsE_oWZ-g02TQzwOFdNtg/edit#slide=id.g9b204bb006_0_30)


#### Note
I had no time yet to explore more complex architecture (transformers, Hopfield network). This will be done in further experiments.
Also, I'm going to focus and reconstructing the full original task of understanding textual information in the complex dataset.
I suspect this to be difficult and one might need a curriculum to achieve a good result.