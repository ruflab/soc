# Testing the fusion module

This experiment explore if the fusion module is able to improve the prediction capacity of our model for the states relying on text information.

### Task
Learn to predict the next state and actions from past states and actions.
One timestep is the tuple (state, action).

Note:
- The game is settlers of catan
- The history length is 1
- The future length is 1
- We are especially looking a accuracy after trade actions. Those states contain meaningfull chat information that should be fusionned with the latent state of the game observation to improve prediction capacities for this particular state-action.

It is interesting to note that without chat information the prediction is stochastic as we do not know what has been traded and have to guess it. On the other hand when chat is added, the prediction becomes deterministic.The chat negotiations contains all there is to know about the trade.

#### On Bert
This experiment is using a pretrained Bert model and embedding to extract interesting features. We do not finetune Bert, instead we use it only as a feature extractor.
As mentionned in the [original paper](https://arxiv.org/pdf/1810.04805.pdf). They are multiple ways to select the interesting feature. Based on Table 7, we will simplify ourselves by using the last hidden layer features. Those consists into contextualised tokens.
One batch of sequences of inputs is preprocessed to end up as a batch of sequences of contextualised tokens.

#### On the Bert feature extractor
1. We will start our experiment with the most simple way to merge the contextualised tokens: a mean.
2. Then, as in the paper, we will try a 2 hidden-layer Bidirectionnal LSTM

#### On fusion modules
1. We will grow the fusion module complexity step by step.
First we will ensure that we can extract needed information from the text, and use a simple concatenation operation. (No fusion actually happen).
2. We will use simple 2 hidden layer NN to fuse information from the game and the text (with residual variation).

#### Merging exctraction and fusion
We will investigate more complex architectures, namely Transformer and the new hopfield layer which are suited to exctract and fusion informations in one step.
1. We will look into the transformer architecture
2. We will use the Hopfield layer.

#### More possibilities
If this experiment ends up not working, we can try to finetune Bert abd use it (using the CLS token) directly as a complete feature extractor.

### Hypothesis
A fusion module can fusion information from text and tabular data to improvate the model predictive capacity.

### Results
